{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOObKC2dx5c4+H8qFwlTeyJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### IMPORTING LIBRARIES"],"metadata":{"id":"nSm2VZdh2Xbr"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from keras.datasets import fashion_mnist\n","import matplotlib.pyplot as plt\n","import random\n","import math\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"V_KIhJq62ijK","executionInfo":{"status":"ok","timestamp":1719829949114,"user_tz":-330,"elapsed":392,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### DATA PREPROCESSING"],"metadata":{"id":"HAm4Oz8Y2m16"}},{"cell_type":"code","source":["(x1,y1),(x2,y2) = fashion_mnist.load_data()\n","x1 = x1 / 255.0\n","x2 = x2 / 255.0\n","x_train1,x_test2,y_train1,y_test2 = train_test_split(x1 , y1 , test_size = 0.1 , random_state = 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FjwvxKdF2wU6","executionInfo":{"status":"ok","timestamp":1719830001464,"user_tz":-330,"elapsed":2993,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}},"outputId":"bb9459b6-ce9c-4a8e-ca4e-56266371167f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","29515/29515 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26421880/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","5148/5148 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4422102/4422102 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["x_train = x_train1.reshape(54000,784).T\n","y1_train = y_train1.reshape(54000,1).T\n","x_test  = x_test2.reshape(6000,784).T\n","y1_test  = y_test2.reshape(6000,1).T\n","print(\"without one-hot encoding\")\n","print(\"shape of x_train :\",x_train.shape)\n","print(\"shape of y_train :\",y1_train.shape)\n","print(\"shape of x_test  :\",x_test.shape)\n","print(\"shape of y_test  :\",y1_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vz6cEZ7V24g6","executionInfo":{"status":"ok","timestamp":1719830029778,"user_tz":-330,"elapsed":381,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}},"outputId":"b1c09088-1835-4ea4-8250-f6d13b419311"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["without one-hot encoding\n","shape of x_train : (784, 54000)\n","shape of y_train : (1, 54000)\n","shape of x_test  : (784, 6000)\n","shape of y_test  : (1, 6000)\n"]}]},{"cell_type":"code","source":["#one hot encoding\n","\n","y_train = np.zeros((10,y1_train.shape[1]))\n","for i in range(0,y1_train.shape[1]):\n","    for j in range(0,10):\n","        if y1_train[0,i] == j:\n","            y_train[j,i] = 1\n","\n","y_test = np.zeros((10,y1_test.shape[1]))\n","for i in range(0,y1_test.shape[1]):\n","    for j in range(0,10):\n","        if y1_test[0,i] == j:\n","            y_test[j,i] = 1\n"],"metadata":{"id":"ChkpcKji27bK","executionInfo":{"status":"ok","timestamp":1719830088253,"user_tz":-330,"elapsed":1366,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### DEFINIG ACTIVATION FUNCTIONS AND LOSS FUNCTIONS"],"metadata":{"id":"H3npH1cs3K5a"}},{"cell_type":"code","source":["# Defining Activation function for neural network\n","\n","def sigmoid_function(x):\n","    exp = np.exp(-x)\n","    return 1/(1+exp)\n","\n","def sigmoid_derivative(x):\n","    return sigmoid_function(x) * (1-sigmoid_function(x))\n","\n","def tanh_function(x):\n","    return np.tanh(x)\n","\n","def tanh_derivative(x):\n","    return (1 - (np.tanh(x)**2))\n","\n","def ReLu(x):\n","    return np.maximum(0,x)\n","\n","def ReLu_derivative(x):\n","    return 1*(x>0)\n","\n","def softmax_function(x):\n","\n","    exps = np.exp(x - np.max(x , axis=0, keepdims = True))\n","    return exps / np.sum(exps, axis=0 , keepdims = True)\n","\n","def softmax_derivative(x):\n","    return softmax_function(x) * (1-(softmax_function(x)))\n","\n","def cost_function(al,y,Batch_size,loss,lamb,parameters):\n","    al = np.clip(al, 1e-9, 1 - 1e-9)                                                    # Clip to avoid taking the log of 0 or 1\n","    if loss == 'cross_entropy':\n","        if y.shape[0] == 1:                                                             # binary classification\n","            cost = (1/Batch_size) * (-np.dot(y,np.log(al).T) - np.dot(1-y, np.log(1-al).T))\n","        else:                                                                           # multiclass-classification\n","            cost = -(1/Batch_size) * np.sum(y * np.log(al))\n","    elif loss == 'mse':\n","         cost = (1/2) * np.sum((y-al)**2)/Batch_size\n","    acc = 0\n","    for i in range(1, len(parameters)//2 + 1):\n","        acc += np.sum(parameters[\"W\"+str(i)]**2)\n","    cost = cost + (lamb/(2*Batch_size))*acc\n","    cost = np.squeeze(cost)\n","    return cost\n"],"metadata":{"id":"ohBTpv-_3QHB","executionInfo":{"status":"ok","timestamp":1719830128739,"user_tz":-330,"elapsed":365,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### INITIALIZING PARAMETERS"],"metadata":{"id":"uFAMcRHF3UKJ"}},{"cell_type":"code","source":["def init_parameters(layers , init_mode):\n","    ''' Function to initialize weights, biases and previous updates of Neural_Network\n","\n","    Parameters\n","    ----------\n","    layers : List\n","        list of numbers of neurons per layer specifying layer dimensions in the format [#inp_features,#num_neurons in layer1,#num_neurons in layer2,......,#out_layer]\n","\n","    init_mode : String\n","        initialization mode ('Random_normal','Random_uniform','Xavier')\n","\n","    Returns\n","    -------\n","    Parameters : Dictionary\n","         contains weights and biases\n","\n","    Previous_Updates : Dictionary\n","         used for different purposes for different optimizers\n","\n","    '''\n","    np.random.seed(42)\n","    Parameters = {}\n","    Previous_Updates = {}\n","    L = len(layers)           #no.of layers\n","\n","    for l in range(1, L):     #except the last activation layer\n","        if init_mode == 'Random_normal':\n","            Parameters['W'+str(l)] = np.random.randn(layers[l],layers[l-1])\n","\n","        elif init_mode == 'Random_uniform':\n","            Parameters['W'+str(l)] = np.random.rand(layers[l],layers[l-1])\n","\n","        elif init_mode == 'Xavier':\n","            Parameters['W'+str(l)] = np.random.randn(layers[l],layers[l-1])*np.sqrt(2/(layers[l]+layers[l-1]))\n","\n","\n","        Parameters['b'+str(l)] = np.zeros((layers[l],1))\n","\n","        Previous_Updates['W'+str(l)] = np.zeros((layers[l],layers[l-1]))\n","        Previous_Updates['b'+str(l)] = np.zeros((layers[l], 1))\n","\n","    return Parameters,Previous_Updates\n"],"metadata":{"id":"CexveMUn3ZVC","executionInfo":{"status":"ok","timestamp":1719830172791,"user_tz":-330,"elapsed":425,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### FORWARD PROPAGATION"],"metadata":{"id":"SKubFbko3gHC"}},{"cell_type":"code","source":["def Forward_Propagation(x, Parameters, activation_function):\n","    '''Function to forward propagate a minibatch of data once through the NN\n","\n","    Parameters\n","    -----------\n","    x: numpy array\n","        data in (features,batch_size) format\n","\n","    Parameters: Dictionary\n","        Weights(W) and biases(b) of the Neural Network\n","\n","    activation_function: String\n","        activation function to be used except the output layer where it takes accordingly(Sigmoid,softmax) based on the type of classification\n","\n","    Returns\n","    --------\n","    output: numpy array\n","        contains the output probabilities for each class and each data sample after only one pass\n","    h: numpy array\n","        contains all post-activations\n","    A: numpy array\n","        contains all pre-activations\n","\n","    '''\n","\n","    forward_prop = {}\n","    L = math.floor(len(Parameters)/2)\n","\n","    #first activation layer will be input layer itself\n","\n","    forward_prop['h0'] = x\n","\n","    # tanh or ReLu activation functions are used for l-1 layers\n","    for l in range(1, L):\n","\n","    # ai+1 = Wi+1 * hij + bi+1\n","        forward_prop['a' + str(l)] = np.dot(Parameters['W' + str(l)],forward_prop['h' + str(l-1)]) + Parameters['b' + str(l)]\n","\n","        if activation_function == 'tanh':\n","            forward_prop['h' + str(l)] = tanh_function(forward_prop['a' + str(l)])\n","        elif activation_function == 'ReLu':\n","            forward_prop['h' + str(l)] = ReLu(forward_prop['a' + str(l)])\n","        elif activation_function == 'sigmoid':\n","            forward_prop['h' + str(l)] = sigmoid_function(forward_prop['a' + str(l)])\n","\n","    forward_prop['a' + str(L)] = np.matmul(Parameters['W' + str(L)],forward_prop['h' + str(L-1)]) + Parameters['b' + str(L)]\n","\n","    # sigmoid or softmax functions are used for output layer\n","    if forward_prop['a' + str(L)].shape[0] == 1:  #if it is a binary output then sigmoid function\n","        forward_prop['h' + str(L)] = sigmoid_function(forward_prop['a' + str(L)])\n","    else :\n","        forward_prop['h' + str(L)] = softmax_function(forward_prop['a' + str(L)]) #if it is multiclass classification then it is softmax function\n","\n","    return forward_prop['h' + str(L)], forward_prop"],"metadata":{"id":"UUMdGthh3key","executionInfo":{"status":"ok","timestamp":1719830224782,"user_tz":-330,"elapsed":383,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### FEED_FORWARD FUNCTION"],"metadata":{"id":"YMf9TRj73yHh"}},{"cell_type":"code","source":["def feed_forward(x,y,layers,init_mode,loss,activation_function):\n","\n","    Parameters,Previous_Updates = init_parameters(layers , init_mode)\n","    al, forward_prop = Forward_Propagation(x, Parameters, activation_function)\n","\n","    return al"],"metadata":{"id":"O8BF1LSr33C6","executionInfo":{"status":"ok","timestamp":1719830288689,"user_tz":-330,"elapsed":386,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["layers = [x_train.shape[0],y_train.shape[0]]\n","num_neuron = [95,90]\n","\n","for i in range(len(num_neuron)):\n","    layers.insert(i+1,num_neuron[i]) # Use num_neuron instead of num\n","for j in range(len(num_neuron)):\n","    print('neurons in hidden layers'+str(j+1) ,num_neuron[j])\n","\n","y_pred = feed_forward(x_train,y_train,layers,init_mode='Xavier',loss='mse',activation_function='sigmoid')\n","print('predicted output :',y_pred[:,0]) #for one data point"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQPpj6mw36Li","executionInfo":{"status":"ok","timestamp":1719830365027,"user_tz":-330,"elapsed":1057,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}},"outputId":"e3f69697-10a2-4f21-d3bf-7f9c35078df6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["neurons in hidden layers1 95\n","neurons in hidden layers2 90\n","predicted output : [0.03896708 0.08704719 0.30094989 0.03719906 0.10418865 0.13478975\n"," 0.21114578 0.01945331 0.02087356 0.04538574]\n"]}]}]}