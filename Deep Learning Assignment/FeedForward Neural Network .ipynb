{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLf/R7fIQGY9l4we9ERjHC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"svjcMNGgT3x5","executionInfo":{"status":"ok","timestamp":1719837747841,"user_tz":-330,"elapsed":21232,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}},"outputId":"c65c3fce-48de-459d-a3cc-172547f45983"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wandb\n","  Downloading wandb-0.17.3-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n","Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-2.7.1-py2.py3-none-any.whl (300 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.2/300.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n","Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.7.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.3\n"]}],"source":["!pip install wandb\n"]},{"cell_type":"code","source":["\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist\n","from keras.datasets import fashion_mnist\n","import copy\n"],"metadata":{"id":"kkHqxetLUTy7","executionInfo":{"status":"ok","timestamp":1719837763056,"user_tz":-330,"elapsed":8554,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def normalize_data(x):\n","  x_norm = x.astype('float32')\n","  x_norm = x_norm / 255.0\n","  return x_norm\n"],"metadata":{"id":"7e81LXOKUaLM","executionInfo":{"status":"ok","timestamp":1719837772515,"user_tz":-330,"elapsed":485,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","\n","def func(activation,a_k1):\n","  a_k = np.clip(a_k1, -55, 55)\n","  if(activation == \"tanh\"):\n","    a_k = np.tanh(a_k)\n","  elif(activation == \"sigmoid\"):\n","    a_k = 1/(1 + np.exp(-1*a_k))\n","  else:\n","    a_k = np.maximum(0,a_k)\n","  return a_k\n"],"metadata":{"id":"rfPGgzp8UdC0","executionInfo":{"status":"ok","timestamp":1719837786622,"user_tz":-330,"elapsed":435,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","def derivativeFun(activation,a_k1):\n","  a_k = np.clip(a_k1, -55, 55)\n","  activationResult = func(activation,a_k)\n","  if(activation == \"tanh\"):\n","    activationResult = 1 - (activationResult**2)\n","  elif(activation == \"sigmoid\"):\n","    activationResult = activationResult - (activationResult**2)\n","  else:\n","    activationResult = np.where(a_k > 0, 1, 0)\n","  return activationResult"],"metadata":{"id":"swNTw1UgUgmy","executionInfo":{"status":"ok","timestamp":1719837798846,"user_tz":-330,"elapsed":438,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","def decision(a_k):\n","  a_k = np.exp(a_k - np.max(a_k))\n","  a_k = a_k / sum(a_k)\n","  return a_k\n"],"metadata":{"id":"nur-vK2yUjoi","executionInfo":{"status":"ok","timestamp":1719837817349,"user_tz":-330,"elapsed":451,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def forwardProp(inputX,activation,weights,bias):\n","  h_k = inputX\n","  PreActivations = list()\n","  PostActivations = list()\n","  PostActivations.append(h_k)\n","  layers = len(weights) - 1\n","  for k in range(0,layers):\n","    a_k = bias[k] + np.dot(weights[k],h_k)\n","    PreActivations.append(a_k)\n","    h_k = func(activation,a_k)\n","    PostActivations.append(h_k)\n","  a_k = bias[layers] + np.matmul(weights[layers],h_k)\n","  PreActivations.append(a_k)\n","  yPred = decision(a_k)\n","  return PreActivations,PostActivations,yPred"],"metadata":{"id":"cnETlY1gUoe7","executionInfo":{"status":"ok","timestamp":1719837828353,"user_tz":-330,"elapsed":448,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def backProp(real, pred, h_k, weights, activation, PreActivations,lossFunction):\n","    a_l_L_theta = pred - real\n","    if lossFunction == \"cross_entropy\":\n","      a_l_L_theta = pred - real\n","    elif lossFunction == \"mean_squared_error\":\n","      a_l_L_theta = np.multiply(np.multiply((pred - real), pred), (1 - pred))\n","\n","    currentActivationGradient = a_l_L_theta\n","    WeightGradients = []\n","    biasGradients = []\n","    layers = len(weights) - 1\n","\n","    for i in range(layers, -1, -1):\n","        W_i_L_theta = currentActivationGradient*np.transpose(h_k[i])\n","        WeightGradients.insert(0, W_i_L_theta)\n","        b_i_L_theta = np.sum(currentActivationGradient, axis=0, keepdims=True)\n","        biasGradients.insert(0, b_i_L_theta)\n","\n","        if i > 0:\n","            h_i_prev_L_theta = np.matmul(weights[i].T, currentActivationGradient)\n","            currentActivationGradient = h_i_prev_L_theta * derivativeFun(activation, PreActivations[i - 1])\n","\n","    return WeightGradients, biasGradients"],"metadata":{"id":"C1hZS50WUqs7","executionInfo":{"status":"ok","timestamp":1719837838884,"user_tz":-330,"elapsed":470,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","def randomizer(dim1,dim2,init_weight):\n","  std_dev = 0.1\n","  if(init_weight == \"Xavier\"):\n","    variance = 2.0 / (dim1 + dim2)\n","    std_dev = np.sqrt(variance)\n","  return std_dev"],"metadata":{"id":"5uQvKaTuUtdr","executionInfo":{"status":"ok","timestamp":1719837849678,"user_tz":-330,"elapsed":458,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","\n","def stochastic_gradient_descent(nodesPerLayer, x_flatten_train, y_encoded, batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n","    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n","    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n","\n","    num_batches = len(x_flatten_train)\n","\n","    for epoch in range(epochs):\n","        for batch in range(0,num_batches):\n","            if(batch % (num_batches/10) == 0):\n","              print(\"batch: \" + str(batch))\n","            start = batch * batch_size\n","            end = (batch + 1) * batch_size\n","\n","            batch_x = x_flatten_train[start:end]\n","            batch_y = y_encoded[start:end]\n","\n","            batch_Wdelta = [np.zeros_like(w) for w in weights]\n","            batch_Bdelta = [np.zeros_like(b) for b in bias]\n","\n","            for j in range(len(batch_x)):\n","                A, B, C = forwardProp(batch_x[j], activationFunc,weights, bias)\n","                Wdelta, Bdelta = backProp(batch_y[j], C, B, weights, activationFunc, A,lossFunction)\n","\n","                for k in range(len(batch_Wdelta)):\n","                    batch_Wdelta[k] += Wdelta[k]\n","                    batch_Bdelta[k] += Bdelta[k]\n","\n","            for k in range(len(weights)):\n","                weights[k] = ((1 - (lr * lambda_reg / batch_size)) * weights[k]) - lr * (batch_Wdelta[k] / batch_size)\n","                bias[k] -= lr * (batch_Bdelta[k] / batch_size)\n","        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n","        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n","        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","\n","    return weights, bias\n","\n"],"metadata":{"id":"S0pdIKTrUwdz","executionInfo":{"status":"ok","timestamp":1719837865928,"user_tz":-330,"elapsed":475,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def momentum_gradient_descent(nodesPerLayer, x_flatten_train, y_encoded, gamma, batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n","\n","    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n","    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n","\n","    Wdelta = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n","    Bdelta = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n","\n","    num_batches = len(x_flatten_train)\n","\n","    for epoch in range(epochs):\n","        for batch in range(0,num_batches):\n","            print(batch)\n","            if(batch % (num_batches/10) == 0):\n","              print(\"batch :\" + str(batch))\n","            start = batch * batch_size\n","            end = (batch + 1) * batch_size\n","\n","            batch_x = x_flatten_train[start:end]\n","            batch_y = y_encoded[start:end]\n","\n","            batch_Wdelta = [np.zeros_like(w) for w in weights]\n","            batch_Bdelta = [np.zeros_like(b) for b in bias]\n","\n","            for j in range(len(batch_x)):\n","                A, B, C = forwardProp(batch_x[j], activationFunc, weights, bias)\n","                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights,activationFunc, A,lossFunction)\n","\n","                for k in range(len(batch_Wdelta)):\n","                    batch_Wdelta[k] += CurrWdelta[k]\n","                    batch_Bdelta[k] += CurrBdelta[k]\n","\n","            for k in range(len(weights)):\n","                Wdelta[k] = gamma * Wdelta[k] + lr * batch_Wdelta[k] / batch_size\n","                Bdelta[k] = gamma * Bdelta[k] + lr * batch_Bdelta[k] / batch_size\n","\n","                weights[k] = ((1 - (lr * lambda_reg / batch_size)) * weights[k]) -Wdelta[k]\n","                bias[k] -= Bdelta[k]\n","        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n","        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n","        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","\n","    return weights, bias\n",""],"metadata":{"id":"doJFp0GeU0Iz","executionInfo":{"status":"ok","timestamp":1719837890624,"user_tz":-330,"elapsed":492,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["\n","\n","def nesterov_gradient_descent(nodesPerLayer,x_flatten_train,y_encoded,gamma, batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n","    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n","    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n","\n","    num_batches = len(x_flatten_train)\n","\n","    for epoch in range(epochs):\n","        for batch in range(0,num_batches):\n","            start = batch * batch_size\n","            end = (batch + 1) * batch_size\n","\n","            batch_x = x_flatten_train[start:end]\n","            batch_y = y_encoded[start:end]\n","\n","            lookahead_weights = [w - gamma * dw for w, dw in zip(weights, weights)]\n","            lookahead_bias = [b - gamma * db for b, db in zip(bias, bias)]\n","\n","            for j in range(len(batch_x)):\n","                A, B, C = forwardProp(batch_x[j], activationFunc, lookahead_weights, lookahead_bias)\n","                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, lookahead_weights, activationFunc, A,lossFunction)\n","\n","                for k in range(len(weights)):\n","                    weights[k] = ((1 - (lr * lambda_reg / batch_size)) * weights[k]) - lr * CurrWdelta[k]\n","                    bias[k] -= lr * CurrBdelta[k]\n","        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n","        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n","        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","    return weights, bias\n",""],"metadata":{"id":"0WpJ9ajgU4mg","executionInfo":{"status":"ok","timestamp":1719837901040,"user_tz":-330,"elapsed":455,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def rmsprop(nodesPerLayer, x_flatten_train, y_encoded, beta, eps, epochs, batch_size, lr,activationFunc,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n","    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n","    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n","\n","    rmsweights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n","    rmsbias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n","\n","    num_batches = len(x_flatten_train)\n","\n","    for epoch in range(epochs):\n","        for batch in range(0,num_batches):\n","            start = batch * batch_size\n","            end = (batch + 1) * batch_size\n","\n","            batch_x = x_flatten_train[start:end]\n","            batch_y = y_encoded[start:end]\n","\n","            batch_w_delta = [np.zeros_like(w) for w in weights]\n","            batch_b_delta = [np.zeros_like(b) for b in bias]\n","\n","            for j in range(len(batch_x)):\n","                A, B, C = forwardProp(batch_x[j], activationFunc,  weights, bias)\n","                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, activationFunc, A,lossFunction)\n","\n","                for k in range(len(CurrWdelta)):\n","                    batch_w_delta[k] += CurrWdelta[k]\n","                    batch_b_delta[k] += CurrBdelta[k]\n","\n","            for k in range(len(batch_w_delta)):\n","                rmsweights[k] = beta * rmsweights[k] + (1 - beta) * (batch_w_delta[k] ** 2)\n","                rmsbias[k] = beta * rmsbias[k] + (1 - beta) * (batch_b_delta[k] ** 2)\n","\n","                weights[k] =((1 - (lr * lambda_reg / batch_size)) * weights[k]) -( (lr * batch_w_delta[k]) / (np.sqrt(rmsweights[k]) + eps))\n","                bias[k] -= (lr * batch_b_delta[k]) / (np.sqrt(rmsbias[k]) + eps)\n","        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n","        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n","        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","\n","    return weights, bias"],"metadata":{"id":"6Xr8WeqVU9AC","executionInfo":{"status":"ok","timestamp":1719837920761,"user_tz":-330,"elapsed":437,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def adam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size, lr,activationFunc,epochs,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n","    # Initialize weights and biases\n","    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n","    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n","\n","    # Initialize Adam parameters\n","    m_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n","    v_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n","    m_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n","    v_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n","\n","    num_batches = len(x_flatten_train)\n","\n","    for epoch in range(epochs):\n","        for batch in range(0,num_batches):\n","            start = batch * batch_size\n","            end = (batch + 1) * batch_size\n","\n","            batch_x = x_flatten_train[start:end]\n","            batch_y = y_encoded[start:end]\n","\n","            batch_w_delta = [np.zeros_like(w) for w in weights]\n","            batch_b_delta = [np.zeros_like(b) for b in bias]\n","\n","            for j in range(len(batch_x)):\n","                A, B, C = forwardProp(batch_x[j], activationFunc,  weights, bias)\n","                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, activationFunc, A,lossFunction)\n","\n","                for k in range(len(CurrWdelta)):\n","                    batch_w_delta[k] += CurrWdelta[k]\n","                    batch_b_delta[k] += CurrBdelta[k]\n","\n","            for k in range(len(batch_w_delta)):\n","                m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * batch_w_delta[k]\n","                v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (batch_w_delta[k] ** 2)\n","                m_bias[k] = beta1 * m_bias[k] + (1 - beta1) * batch_b_delta[k]\n","                v_bias[k] = beta2 * v_bias[k] + (1 - beta2) * (batch_b_delta[k] ** 2)\n","\n","                m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n","                v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n","                m_bias_hat = m_bias[k] / (1 - beta1 ** (epoch + 1))\n","                v_bias_hat = v_bias[k] / (1 - beta2 ** (epoch + 1))\n","\n","                weights[k] = ((1 - (lr * lambda_reg / batch_size)) * weights[k]) - ( (lr * m_weights_hat) / (np.sqrt(v_weights_hat) + eps))\n","                bias[k] -= (lr * m_bias_hat) / (np.sqrt(v_bias_hat) + eps)\n","\n","        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n","        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n","        #print({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","\n","\n","    return weights, bias"],"metadata":{"id":"lvZMews4VBwq","executionInfo":{"status":"ok","timestamp":1719837932872,"user_tz":-330,"elapsed":463,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def nadam(nodesPerLayer, x_flatten_train, y_encoded, beta1, beta2, eps, batch_size, lr,activationFunc,epochs,x_flatten_test,y_flatten_test,x_val,y_val,init_weight,lambda_reg,lossFunction):\n","    # Initialize weights and biases\n","    weights = [np.random.randn(nodesPerLayer[i], nodesPerLayer[i-1]) * randomizer(nodesPerLayer[i], nodesPerLayer[i-1],init_weight) for i in range(1, len(nodesPerLayer))]\n","    bias = [np.random.randn(nodesPerLayer[i], 1) * 0.1 for i in range(1, len(nodesPerLayer))]\n","\n","    # Initialize Nadam parameters\n","    m_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n","    v_weights = [np.zeros((nodesPerLayer[i], nodesPerLayer[i-1])) for i in range(1, len(nodesPerLayer))]\n","    m_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n","    v_bias = [np.zeros((nodesPerLayer[i], 1)) for i in range(1, len(nodesPerLayer))]\n","\n","    num_batches = len(x_flatten_train) // batch_size\n","\n","    for epoch in range(epochs):\n","        for batch in range(0,num_batches):\n","            start = batch * batch_size\n","            end = (batch + 1) * batch_size\n","            batch_x = x_flatten_train[start:end]\n","            batch_y = y_encoded[start:end]\n","\n","            batch_w_delta = [np.zeros_like(w) for w in weights]\n","            batch_b_delta = [np.zeros_like(b) for b in bias]\n","\n","            for j in range(len(batch_x)):\n","                A, B, C = forwardProp(batch_x[j], activationFunc, weights, bias)\n","                CurrWdelta, CurrBdelta = backProp(batch_y[j], C, B, weights, activationFunc, A,lossFunction)\n","\n","                for k in range(len(CurrWdelta)):\n","                    batch_w_delta[k] += CurrWdelta[k]\n","                    batch_b_delta[k] += CurrBdelta[k]\n","\n","            for k in range(len(batch_w_delta)):\n","                m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * batch_w_delta[k]\n","                v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (batch_w_delta[k] ** 2)\n","                m_bias[k] = beta1 * m_bias[k] + (1 - beta1) * batch_b_delta[k]\n","                v_bias[k] = beta2 * v_bias[k] + (1 - beta2) * (batch_b_delta[k] ** 2)\n","\n","                m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n","                v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n","                m_bias_hat = m_bias[k] / (1 - beta1 ** (epoch + 1))\n","                v_bias_hat = v_bias[k] / (1 - beta2 ** (epoch + 1))\n","\n","                weights[k] =((1 - (lr * lambda_reg / batch_size)) * weights[k]) - ( lr * (beta1 * m_weights_hat + (1 - beta1) * batch_w_delta[k]) / (np.sqrt(v_weights_hat) + eps))\n","                bias[k] -= lr * (beta1 * m_bias_hat + (1 - beta1) * batch_b_delta[k]) / (np.sqrt(v_bias_hat) + eps)\n","        accuracy,loss = testModel(weights,bias,x_flatten_test,y_flatten_test,activationFunc,lossFunction)\n","        Valaccuracy,Valloss = testModel(weights,bias,x_val,y_val,activationFunc,lossFunction)\n","        wandb.log({\"val_loss\":Valloss,\"val_accuracy\":Valaccuracy,\"loss\":loss,\"accuracy\":accuracy,\"epoch\":epoch})\n","\n","    return weights, bias\n"],"metadata":{"id":"Duhhhv3ZVFpj","executionInfo":{"status":"ok","timestamp":1719837944959,"user_tz":-330,"elapsed":1,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["\n","\n","def gradient_descent(nodesPerLayer,x_flatten_train,y_encoded,activationFunc,epochs,lr):\n","  weights = list()\n","  bias = list()\n","  w = np.random.randn(nodesPerLayer[i],nodesPerLayer[i-1])*0.1\n","  b =  np.random.randn(nodesPerLayer[i],1)\n","  weights.append(w)\n","  bias.append(b)\n","  Wdelta = list()\n","  Bdelta = list()\n","  for i in range(0,epochs):\n","    Wdelta.clear()\n","    Bdelta.clear()\n","    for j in range(0,len(y_encoded)):\n","      A,B,C = forwardProp(x_flatten_train[j],activationFunc,weights,bias)\n","      CurrWdelta,CurrBdelta = backProp(y_encoded[j],C,B,weights,activationFunc,A)\n","      if( len(Wdelta) == 0):\n","        Wdelta =  copy.deepcopy(CurrWdelta)\n","        Bdelta = copy.deepcopy(CurrBdelta)\n","      else:\n","        for k in range(0,len(Wdelta)):\n","          Wdelta[k] = Wdelta[k] + CurrWdelta[k]\n","          Bdelta[k] = Bdelta[k] + CurrBdelta[k]\n","      if(j%1000 == 0):\n","        print(j/1000)\n","    for k in range(0,len(weights)):\n","      weights[k] = weights[k] - lr*Wdelta[k]\n","      bias[k] = bias[k] - lr*Bdelta[k]\n","  return weights,bias\n","\n",""],"metadata":{"id":"6lEV6zEZVIbc","executionInfo":{"status":"ok","timestamp":1719837958187,"user_tz":-330,"elapsed":682,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["\n","def executeTraining(config,x_train,y_train,x_flatten_test,y_flatten_test,x_val,y_val):\n","  FinalWeights = list()\n","  FinalBias = list()\n","  beta1 = 0.8\n","  beta2 = 0.9\n","  eps = 1e-6\n","  batch_size = config.batch_size\n","  layers = config.number_hidden\n","  lr = config.learning_rate\n","  epochs = config.epochs\n","  activationFunc = config.activation\n","  optimizer = config.optimizer\n","  weightInit = config.weight_init\n","  lambda_reg = config.weight_decay\n","  nodesPerLayer = list()\n","  nodesPerLayer.append(784)\n","  for i in range(0,layers):\n","    nodesPerLayer.append(config.number_hidden)\n","  nodesPerLayer.append(10)\n","\n","  lossFunction = \"cross_entropy\"\n","  gamma = 0.00001\n","  betarms = 0.001\n","  if(optimizer == \"gradient_descent\"):\n","    FinalWeights, FinalBias = gradient_descent(nodesPerLayer,x_train,y_train,activationFunc,epochs,lr)\n","  elif(optimizer == \"sgd\"):\n","    FinalWeights, FinalBias = stochastic_gradient_descent(nodesPerLayer,x_train,y_train,batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n","  elif(optimizer == \"momentum\"):\n","    FinalWeights, FinalBias = momentum_gradient_descent(nodesPerLayer,x_train,y_train,gamma,batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n","  elif(optimizer == \"nag\"):\n","    FinalWeights, FinalBias = nesterov_gradient_descent(nodesPerLayer,x_train,y_train,gamma,batch_size,activationFunc,epochs,lr,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n","  elif(optimizer == \"rmsprop\"):\n","    FinalWeights, FinalBias =rmsprop(nodesPerLayer,x_train,y_train,betarms,eps,epochs,batch_size,lr,activationFunc,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n","  elif(optimizer == \"adam\"):\n","    FinalWeights, FinalBias = adam(nodesPerLayer, x_train, y_train, beta1, beta2, eps, batch_size,lr,activationFunc,epochs,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n","  elif(optimizer == \"nadam\"):\n","    FinalWeights, FinalBias = nadam(nodesPerLayer, x_train, y_train, beta1, beta2, eps, batch_size,lr,activationFunc,epochs,x_flatten_test,y_flatten_test,x_val,y_val,weightInit,lambda_reg,lossFunction)\n","  return FinalWeights,FinalBias\n","\n"],"metadata":{"id":"pO918iydVKYb","executionInfo":{"status":"ok","timestamp":1719837971308,"user_tz":-330,"elapsed":443,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["\n","\n","def testModel(weights,bias,x_test,y_test,activationFun,lossFunction):\n","  count = 0\n","  loss = 0.0\n","\n","  for i in range(0,x_test.shape[0]):\n","    A,B,C = forwardProp(x_test[i],activationFun,weights,bias)\n","    if( y_test[i]==np.argmax(C)):\n","      count+=1\n","\n","    if(lossFunction == \"mean_squared_error\"):\n","      loss += (np.argmax(C) -  y_test[i])**2\n","    else:\n","      loss += -np.log(C)[y_test[i]][0]\n","\n","  loss /= y_test.shape[0]\n","  acc = (count/y_test.shape[0])\n","  return acc,loss\n","\n"],"metadata":{"id":"w5h2U4eOVN1b","executionInfo":{"status":"ok","timestamp":1719837986368,"user_tz":-330,"elapsed":477,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["\n","\n","def import_data(dataset):\n","  if(dataset == \"mnist\"):\n","   return mnist.load_data()\n","  else:\n","   return fashion_mnist.load_data()\n","\n",""],"metadata":{"id":"XibtiAsQVRZD","executionInfo":{"status":"ok","timestamp":1719838003645,"user_tz":-330,"elapsed":453,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["\n","sweep_config = {\n","    'name':\"my-sweep\",\n","    'method': 'bayes',\n","    'metric': {\n","      'name': 'accuracy',\n","      'goal': 'maximize'\n","    },\n","\n","    'parameters': {\n","        'epochs': {\n","            'values': [5, 10] #number of epochs\n","        },\n","        'number_hidden': {\n","            'values': [3, 4, 5] #number of hidden layers\n","        },\n","        'hidden_inputsize': {\n","            'values':[32, 64, 128] #size of every hidden layer\n","        },\n","        'weight_decay': {\n","            'values':[0, 0.0005,  0.5] #L2 regularisation\n","        },\n","        'learning_rate': {\n","            'values': [1e-3, 1e-4]\n","        },\n","        'optimizer': {\n","            'values': ['momentum']\n","        }, #'nag', 'rmsprop', 'adam', 'nadam','sgd'\n","        'batch_size' : {\n","            'values':[16, 32, 64]\n","        },\n","        'weight_init': {\n","            'values':['random','Xavier']\n","        },\n","        'activation': {\n","            'values': ['sigmoid','tanh','relu']\n","        }\n","\n","        }\n","}\n"],"metadata":{"id":"EpWszTnaVVrb","executionInfo":{"status":"ok","timestamp":1719838017412,"user_tz":-330,"elapsed":443,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def load_data(dataset):\n","\n","  (x_train, y_train), (x_test, y_test) = import_data(dataset)\n","  x_train, y_train = shuffle(x_train, y_train)\n","  val_size = int(x_train.shape[0] * 0.1)\n","\n","  y_val = y_train[:val_size]\n","  y_new_train = y_train[val_size:]\n","\n","  x_flatten_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2],1)\n","  x_flatten_train = normalize_data(x_flatten_train)\n","\n","  y_encoded = np.zeros((y_new_train.shape[0], 10))\n","  y_encoded[np.arange(y_new_train.shape[0]), y_new_train] = 1\n","  y_new_train = y_encoded.reshape(y_new_train.shape[0],10,1)\n","\n","\n","  x_flatten_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2],1)\n","  x_flatten_test = normalize_data(x_flatten_test)\n","\n","\n","  x_val = x_flatten_train[:val_size]\n","\n","\n","  x_new_train = x_flatten_train[val_size:]\n","\n","\n","  return x_new_train, y_new_train,x_val,y_val,x_flatten_test,y_test\n","\n"],"metadata":{"id":"qwSxI0WcVZAT","executionInfo":{"status":"ok","timestamp":1719838033428,"user_tz":-330,"elapsed":450,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","def plot_diffClasses():\n","  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","  names = [\"T-shirt\",\"Trouser\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n","  variousSamples = list()\n","  classes = set(y_train)\n","  for i in classes:\n","    ind = np.where(y_train == i)[0][0]\n","    variousSamples.append(wandb.Image(x_train[ind],caption = names[i]))\n","\n","  wandb.log({\"examples\": variousSamples})\n","\n","\n"],"metadata":{"id":"e4CjtuefVczj","executionInfo":{"status":"ok","timestamp":1719838046810,"user_tz":-330,"elapsed":445,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["\n","\n","def GenConfusion(weights,bias,x_test,y_test,activationFun):\n","  prediction = list()\n","  names = [\"T-shirt\",\"Trouser\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n","  for i in range(0,x_test.shape[0]):\n","    A,B,C = forwardProp(x_test[i],activationFun,\"crossEntropy\",weights,bias)\n","    prediction.append(np.argmax(C))\n","  wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(y_true=y_test, preds=prediction,class_names=names)})\n","  return\n","\n"],"metadata":{"id":"yQ3ugFvBVhKD","executionInfo":{"status":"ok","timestamp":1719838057940,"user_tz":-330,"elapsed":469,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["\n","def fitModel():\n","  with wandb.init() as run:\n","    config = wandb.config\n","    wandb.run.name = \"hidden_\" + str(config.hidden_inputsize)+\"_batchSize_\"+str(config.batch_size)+\"_acc_\"+ config.activation\n","    np.random.seed(1)\n","\n","    x_new_train, y_new_train,x_val,y_val,x_flatten_test,y_flatten_test = load_data(\"fashion_mnist\")\n","    trainedWeights,trainedBias = executeTraining(config,x_new_train,y_new_train,x_flatten_test,y_flatten_test,x_val,y_val)\n"],"metadata":{"id":"cr3vewQJVjf7","executionInfo":{"status":"ok","timestamp":1719838067546,"user_tz":-330,"elapsed":441,"user":{"displayName":"Sanjay S","userId":"16697160973224283713"}}},"execution_count":26,"outputs":[]}]}